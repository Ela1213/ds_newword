import os
import re
import time
import json
import pandas as pd
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential

# æ·»åŠ ç¯å¢ƒå˜é‡å¤„ç†
import sys
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    logger.error("DeepSeek APIå¯†é’¥æœªè®¾ç½®! è¯·é€šè¿‡ç¯å¢ƒå˜é‡DEEPSEEK_API_KEYæä¾›")
    sys.exit(1)

# å…¶ä»–é…ç½®
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "20"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))

# åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com"
)

# è¿›åº¦æ–‡ä»¶è·¯å¾„
PROGRESS_FILE = "progress.json"
RESULTS_DIR = "results"
ERRORS_DIR = "errors"

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(ERRORS_DIR, exist_ok=True)

def load_progress():
    """åŠ è½½å¤„ç†è¿›åº¦"""
    try:
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {"last_index": 0, "processed": 0, "batches": []}

def save_progress(progress):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)

def load_wordlist():
    """åŠ è½½è¯è¡¨æ–‡ä»¶"""
    # å°è¯•ä¸åŒå¯èƒ½çš„è·¯å¾„
    possible_paths = [
        "src/wordlist/sample_new_words.csv",
        "wordlist/sample_new_words.csv",
        "sample_new_words.csv"
    ]
    
    for path in possible_paths:
        try:
            df = pd.read_csv(path, encoding="utf-8-sig")
            print(f"âœ… æˆåŠŸåŠ è½½è¯è¡¨: {path}")
            return df["word"].tolist()
        except FileNotFoundError:
            continue
    
    raise FileNotFoundError("æœªæ‰¾åˆ°è¯è¡¨æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶ä½äº src/wordlist/ æˆ–é¡¹ç›®æ ¹ç›®å½•")

def build_prompt(word_batch):
    """æ„å»ºæç¤ºè¯æ¨¡æ¿"""
    prompt = """ä½ æ˜¯ä¸€ä½ä¸­æ–‡è¯­è¨€æ¼”åŒ–ä¸å¿ƒç†è¯­è¨€å­¦ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°ç»™å®šè¯è¯­æ˜¯å¦ä¸º2008å¹´åå‡ºç°çš„æ–°è¯ã€‚ç‰¹åˆ«æ³¨æ„é¿å…è®¤çŸ¥åå·®ï¼Œé€šè¿‡è¯¥è¯åœ¨2008å¹´å‰çš„å«ä¹‰å®¢è§‚å¯¹æ¯”å˜åŒ–ï¼Œä¼˜å…ˆåŸºäºè¯­ä¹‰å’Œè¯­ç”¨åŠŸèƒ½çš„å®è´¨æ€§è¯æ®ï¼ˆå¦‚è¯­æ–™åº“æˆ–æ¼”åŒ–ç ”ç©¶ï¼‰ï¼Œè€Œéé«˜é¢‘å°è±¡ã€‚åœ¨åˆ¤æ–­æ—¶ï¼Œç§¯æè€ƒè™‘åä¾‹ï¼ˆå¦‚å†å²ç”¨ä¾‹æˆ–æœªè¢«æ¥å—çš„æ–°ä¹‰ï¼‰ï¼ŒæŒ‘æˆ˜åˆæ­¥å‡è®¾ï¼Œç¡®ä¿ç»“è®ºåŸºäºå®¢è§‚è¯æ®è€Œéé¢„è®¾åå¥½ã€‚ä¸¥æ ¼æŒ‰é¡ºåºå®Œæˆä»¥ä¸‹æ­¥éª¤ï¼Œç¦æ­¢è·³æ­¥ï¼š

æ­¥éª¤1ï¼šè¯­ä¹‰å˜åŒ–åˆ¤æ–­
   - åäº‹å®æ€è€ƒï¼šå¯¹æ¯”2008å¹´å‰åçš„æ ¸å¿ƒå«ä¹‰ï¼Œæ˜¯å¦æœ‰æ–°çš„æ ¸å¿ƒå«ä¹‰æˆ–è¯­ç”¨åŠŸèƒ½ï¼ˆå¿…é¡»æ˜¯å«ä¹‰çš„æœ¬è´¨å˜åŒ–ï¼Œè€Œéä»…åœºæ™¯/è½½ä½“å˜åŒ–çš„é¢†åŸŸå»¶ä¼¸ï¼Œå¦‚â€œåˆ·â€ä»â€œåˆ·ç‰™â€åˆ°â€œåˆ·æ‰‹æœºâ€æ˜¯åœºæ™¯å˜åŒ–ï¼Œä½†æ ¸å¿ƒâ€œå¿«é€Ÿæ“ä½œâ€ä¸å˜ï¼›è€Œâ€œèººå¹³â€æ–°å¢â€œæ¶ˆææŠµæŠ—â€å«ä¹‰å±æœ¬è´¨å˜åŒ–ï¼‰ã€‚
   - å¦‚æ˜¯ï¼Œè¿›å…¥â€œA æ—§è¯æ–°ä¹‰â€å€™é€‰ï¼›å¦‚å¦ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥ã€‚

æ­¥éª¤2ï¼šå½¢å¼åˆ¤æ–­
   - åˆ¤æ–­è¯¥è¯å½¢å¼æ˜¯å¦ä¸ºæ–°é€ ï¼ˆåŒ…æ‹¬æ„è¯åˆ›æ–°ã€ä¸­è‹±æ–‡ç¼©ç•¥ã€è°éŸ³ã€æ‹¼éŸ³ã€èˆ¶æ¥ç­‰ï¼‰ï¼Œå³ç»„åˆæ–¹å¼ä¸ç¬¦åˆä¼ ç»Ÿæ„è¯æ³•ï¼Œæˆ–è€…å«ä¹‰æ— æ³•ä»å­—é¢æ¨å¯¼ã€‚
   - å¦‚æ˜¯ï¼Œè¿›å…¥â€œB æ–°è¯æ–°ä¹‰â€å€™é€‰ï¼›å¦‚å¦ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥ã€‚

æ­¥éª¤3ï¼šæ’é™¤æ³•
   - è‹¥æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼Œè€ƒè™‘åˆ¤ä¸ºâ€œC éæ–°è¯â€ï¼š
     1) å½¢å¼ä¸å«ä¹‰å‡æ²¿ç”¨ä¼ ç»Ÿï¼ˆå¦‚â€œä¹¦â€æ— è®ºçº¸è´¨æˆ–ç”µå­ï¼Œæ ¸å¿ƒâ€œé˜…è¯»è½½ä½“â€ä¸å˜ï¼‰ã€‚
     2) æ ¸å¿ƒå«ä¹‰æœªå˜ï¼Œä»…åœºæ™¯è¿ç§»/è½½ä½“å˜åŒ–/æ­é…æ‹“å±•ï¼ˆå¦‚â€œçª—å£â€ä»ç‰©ä½“åˆ°ç”µå­çš„èŠå¤©çª—å£ï¼Œæ ¸å¿ƒå«ä¹‰ä¸å˜ï¼‰ã€‚
     3) æ–°å‡ºç°çš„ä¸“æœ‰åè¯ï¼Œä¸”æœªè¡ç”Ÿå‡ºæ™®é€šè¯æ±‡ç”¨æ³•ï¼ˆå“ç‰Œã€äººåã€åœ°åã€ä½œå“åç­‰ï¼Œå¦‚â€œå¾®ä¿¡â€è™½æ–°ä½†å±å“ç‰Œåï¼‰ã€‚
     4) ç”±æ”¿åºœ/å¹¿å‘Šæ¨å‡ºä½†æœªè¢«æ°‘é—´å¹¿æ³›ä½¿ç”¨ï¼Œæœªå‘ç”Ÿè¯­ä¹‰/å½¢å¼åˆ›æ–°ï¼ˆå¦‚å®˜æ–¹æœ¯è¯­â€œå¯¡å¤´â€è¯­ä¹‰ä¸å˜ï¼‰ã€‚
     5) æ— å¯é è¯æ®æ”¯æŒè¯­ä¹‰æˆ–å½¢å¼æ–°å˜ï¼ˆåŸºäºæ­¥éª¤1-2åˆ†æï¼‰ã€‚

æ­¥éª¤4ï¼šè¾¹ç•Œåˆ¤æ–­
   - å¦‚æœåˆ†æåä»ä¸ç¡®å®šï¼ˆå¦‚è¯æ®å†²çªã€ä½¿ç”¨èŒƒå›´æœ‰é™ï¼‰ï¼Œæˆ–è¯¥è¯ä½äºæ–°æ—§äº¤ç•Œï¼ˆå¦‚æ—§è¯æ–°ä¹‰ä½†å˜åŒ–è½»å¾®ï¼‰ï¼Œæ ‡è®°ä¸ºè¾¹ç•Œè¯ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ä¿æŒé¡ºåºå’Œè¡Œæ•°ï¼Œä¸æ·»åŠ å¤šä½™è§£é‡Šï¼‰ï¼š
åŸå§‹è¯è¯­

åˆ†ç±»ç®€è¦ç†ç”±ï¼ˆå…ˆé™ˆè¿°åäº‹å®æ€è€ƒçš„è¯­ä¹‰å¯¹æ¯”å’Œå½¢å¼åˆ†æï¼Œå†è¯´æ˜æ’é™¤æˆ–å½’ç±»åŸå› ï¼‰

ä¸¤ä¸ªè¯­ä¹‰é‚»è¿‘è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰

åˆ†ç±»ç»“æœï¼ˆA / B / Cï¼‰

å¯¹â€œæ˜¯å¦ä¸ºAæˆ–Bç±»æ–°è¯â€çš„åˆ¤æ–­ç½®ä¿¡åº¦ï¼ˆ1-10ï¼ŒåŸºäºè¯æ®å¼ºåº¦ï¼š10=é«˜è¯æ®ï¼Œå¦‚å¤šä¸ªè¯­æ–™æ¥æºï¼›1=ä½è¯æ®ï¼Œå¦‚ä¾èµ–æ¨æµ‹ï¼‰

æ˜¯å¦ä¸ºè¾¹ç•Œè¯ï¼ˆæ˜¯ / å¦ï¼‰

è¾“å‡ºç¤ºä¾‹
æ‘¸é±¼
åŸè¡¨ç¤ºâ€œæ•é±¼â€ï¼Œå½“å‰æ–°å¢â€œå·¥ä½œä¸­å·æ‡’â€çš„å«ä¹‰ï¼Œå±è¯­ä¹‰æœ¬è´¨å˜åŒ–ï¼Œå½¢å¼ä¸ºæ—§ã€‚
å·æ‡’,åˆ’æ°´
A
9
å¦

- æ¯ç»„è¯ä¹‹é—´ç”¨æ¢è¡Œåˆ†éš”ã€‚
- ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—æˆ–ç¼–å·ã€‚ä¿æŒè¾“å‡ºç»“æ„ç»Ÿä¸€ã€‚
"""

    for i, word in enumerate(word_batch, 1):
        prompt += f"\nè¯è¯­ {i}ï¼š{word}"
    return prompt

@retry(stop=stop_after_attempt(MAX_RETRIES), wait=wait_random_exponential(multiplier=1, max=60))
def process_batch(batch, batch_num):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
    prompt = build_prompt(batch)
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸­æ–‡è¯­è¨€å­¦å®¶"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=10000
        )
        return response.choices[0].message.content
    except Exception as e:
        raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")

def parse_response(response_text, batch):
    """è§£æAPIå“åº”å¹¶å¤„ç†é”™è¯¯"""
    entries = []
    raw_blocks = response_text.strip().split('\n\n')
    
    # æœ‰æ•ˆå“åº”æ¨¡å¼
    valid_pattern = re.compile(r".+\n.+\n.+\n[A-C]\n\d+\n(æ˜¯|å¦)")
    
    for idx, block in enumerate(raw_blocks):
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå“åº”æ ¼å¼
            block = block.strip()
            if not valid_pattern.match(block):
                raise ValueError("å“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                
            lines = block.split('\n')
            if len(lines) < 6:
                raise ValueError("å“åº”è¡Œæ•°ä¸è¶³")
                
            entry = {
                "word": batch[idx] if idx < len(batch) else f"UNKNOWN_{idx}",
                "reason": lines[1],
                "near_words": lines[2],
                "category": lines[3],
                "confidence": lines[4],
                "is_boundary": lines[5],
                "raw_response": block
            }
            entries.append(entry)
        except Exception as e:
            # æ ‡è®°é”™è¯¯æ¡ç›®
            word = batch[idx] if idx < len(batch) else f"UNKNOWN_{idx}"
            entries.append({
                "word": word,
                "error": f"è§£æé”™è¯¯: {str(e)}",
                "raw_response": block
            })
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„è¯è¯­
    processed_words = {entry["word"] for entry in entries}
    for word in batch:
        if word not in processed_words:
            entries.append({
                "word": word,
                "error": "æœªåœ¨å“åº”ä¸­æ‰¾åˆ°å¯¹åº”ç»“æœ",
                "raw_response": response_text[:200] + "..."  # æˆªå–éƒ¨åˆ†å“åº”
            })
    
    return entries

def save_results(entries, batch_num):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    filename = f"{RESULTS_DIR}/batch_{batch_num}.csv"
    df = pd.DataFrame(entries)
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    return filename

def save_errors(entries, batch_num):
    """ä¿å­˜é”™è¯¯ä¿¡æ¯"""
    filename = f"{ERRORS_DIR}/batch_{batch_num}_errors.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)
    return filename

def aggregate_results():
    """èšåˆæ‰€æœ‰æ‰¹æ¬¡ç»“æœ"""
    all_entries = []
    
    # éå†ç»“æœç›®å½•
    for filename in os.listdir(RESULTS_DIR):
        if filename.endswith(".csv"):
            filepath = os.path.join(RESULTS_DIR, filename)
            df = pd.read_csv(filepath)
            all_entries.extend(df.to_dict("records"))
    
    # åˆ›å»ºæœ€ç»ˆç»“æœæ–‡ä»¶
    final_filename = "final_results.csv"
    pd.DataFrame(all_entries).to_csv(final_filename, index=False, encoding="utf-8-sig")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_words = len(all_entries)
    success_words = sum(1 for e in all_entries if "error" not in e or pd.isna(e.get("error")))
    error_words = total_words - success_words
    
    return {
        "filename": final_filename,
        "total_words": total_words,
        "success_words": success_words,
        "error_words": error_words
    }

def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤„ç†è¯è¡¨...")
    start_time = time.time()
    
    # åŠ è½½è¯è¡¨
    words = load_wordlist()
    total_words = len(words)
    print(f"ğŸ“‹ è¯è¡¨åŠ è½½å®Œæˆï¼Œå…± {total_words} ä¸ªè¯è¯­")
    
    # åŠ è½½è¿›åº¦
    progress = load_progress()
    start_index = progress["last_index"]
    processed_batches = set(progress["batches"])
    
    print(f"â±ï¸ ä»ç¬¬ {start_index} ä¸ªè¯è¯­ç»§ç»­å¤„ç†...")
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(start_index, total_words, BATCH_SIZE):
        batch_num = (i // BATCH_SIZE) + 1
        
        # è·³è¿‡å·²å¤„ç†çš„æ‰¹æ¬¡
        if batch_num in processed_batches:
            print(f"â­ï¸ æ‰¹æ¬¡ #{batch_num} å·²å¤„ç†ï¼Œè·³è¿‡")
            continue
            
        batch = words[i:i+BATCH_SIZE]
        print(f"\nğŸ” å¤„ç†æ‰¹æ¬¡ #{batch_num}: {batch}")
        
        try:
            # å¤„ç†æ‰¹æ¬¡
            response_text = process_batch(batch, batch_num)
            
            # è§£æç»“æœ
            entries = parse_response(response_text, batch)
            
            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
            success_count = sum(1 for e in entries if "error" not in e)
            error_count = len(entries) - success_count
            
            # ä¿å­˜ç»“æœ
            result_file = save_results(entries, batch_num)
            print(f"âœ… æ‰¹æ¬¡ #{batch_num} å®Œæˆ! æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
            print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {result_file}")
            
            # å¦‚æœæœ‰é”™è¯¯ï¼Œå•ç‹¬ä¿å­˜é”™è¯¯ä¿¡æ¯
            if error_count > 0:
                error_entries = [e for e in entries if "error" in e]
                error_file = save_errors(error_entries, batch_num)
                print(f"âš ï¸ å‘ç° {error_count} ä¸ªé”™è¯¯ï¼Œä¿å­˜è‡³: {error_file}")
            
            # æ›´æ–°è¿›åº¦
            progress["last_index"] = i + BATCH_SIZE
            progress["processed"] += len(batch)
            progress["batches"].append(batch_num)
            save_progress(progress)
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ #{batch_num} å¤±è´¥: {str(e)}")
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            error_entries = [{"word": w, "error": str(e)} for w in batch]
            error_file = save_errors(error_entries, batch_num)
            print(f"ğŸ’¾ é”™è¯¯ä¿¡æ¯ä¿å­˜è‡³: {error_file}")
            
            # æ›´æ–°è¿›åº¦ï¼ˆè·³è¿‡å½“å‰æ‰¹æ¬¡ï¼‰
            progress["last_index"] = i + BATCH_SIZE
            progress["batches"].append(batch_num)
            save_progress(progress)
        
        # é¿å…é€Ÿç‡é™åˆ¶
        time.sleep(1)
    
    # èšåˆç»“æœ
    if progress["last_index"] >= total_words:
        print("\nâœ¨ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ! æ­£åœ¨èšåˆç»“æœ...")
        agg_result = aggregate_results()
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æ€»è¯è¯­ {agg_result['total_words']}, "
              f"æˆåŠŸ {agg_result['success_words']}, "
              f"å¤±è´¥ {agg_result['error_words']}")
        print(f"ğŸ’¾ æœ€ç»ˆç»“æœä¿å­˜è‡³: {agg_result['filename']}")
    
    # è®¡ç®—æ€»è€—æ—¶
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    print("âœ… å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()
